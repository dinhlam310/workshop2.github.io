[
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/1-introduce/1.1-setup/1.1.1-awsworkshopstudio/",
	"title": "AWS Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Introduction We will walk through the steps to get started with the AWS Workshop Studio environment. If you are conducting this lab on your own, or an AWS account is not being provided to you, please follow the Self-Paced Workshop instructions.\nFor events that use AWS Workshop Studio, you will be provided with a 12-character \u0026ldquo;hash\u0026rdquo; that you will use to gain access to a dedicated AWS account for the purposes of this workshop.\nSteps Navigate to the AWS Workshop Studio and click Get started. Click Email one-time password (OTP) Enter your email address and click Send passcode Enter the access code you received from the email you designated in the previous step Enter the Event Access Code provided by the workshop facilitators Accept the Terms and Conditions and click Join event Workshop Landing Page Once you have logged into the AWS Workshop Studio for your specific event, you will be automatically directed to the general landing page for the lab. From here you can review all lab instructions, access the AWS account provided to you via the console or the AWS CLI.\nClick Open AWS console in the left hand menu under AWS account access section to access the AWS account provided to you, you will complete all instructions in this account. Once Workshop Studio setup is complete:\nClick here to jump to the first lab. Clicking Next will take you to setup instructions for the Self-Paced workshop experience.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/3-supportknowledgebase/3.1-createknowledgebase/",
	"title": "Create a Knowledge Base",
	"tags": [],
	"description": "",
	"content": "Introduction In this step, we will be creating an Amazon Bedrock: Knowledge Base. The knowledge base will reference an S3 data source that consists of support and product data for both Ohmzio and Ampwerks.\nSteps Navigate to the Amazon Bedrock console . Click Get Started If presented with Welcome to Amazon Bedrock! pop-up, proceed by clicking Manage model access. If not, proceed to next step. Request model access: On the left of the screen, click the menu icon to open the services menu.\nClick Model access\nIn the Model access menu, click Enable specific models / Modify model access.\nSelect the following Base models:\nTitan Text Embeddings V2 Claude 3.5 Sonnet Please note some base models listed may already have Access granted. This will not affect your ability to complete the lab. If a model listed in this step is already granted, proceed requesting access to all other required models.\nClick Next.\nIn the Review and submit page click Submit. Amazon Bedrock provides a turnkey experience for users. For example, users are able to leverage several types of foundational models across all of the Bedrock services. This, in turn, provides users the option to select a foundational model that best suits their needs. However, foundational model providers (i.e. Anthropic, Cohere, Stability AI, etc.) require EULA acceptance and reasonable guardrails for model usage prior to implementation. The Model Access console manages these agreements and enables administrators to enforce the principle of least privilage. To learn more about foundational models refer back to Amazon Bedrock: Knowledge Base key concepts.\nAfter requesting model access, Amazon Bedrock is doing the \u0026ldquo;undifferentiated heavy lifting\u0026rdquo; of managing access to these open-source foundational models. Granting access to non-Amazon foundational models may take a few minutes. To verify access has been granted, click the refresh button next to Manage model access.\nClick Knowledge bases in the left menu under the Builder tools section.\nIn the Knowledge bases landing page, click Create knowledge base.\nStep 1, provide knowledge base details:\nName the knowledge base: merger-knowledge-base Describe the knowledge base: Knowledge base for product and support data for both Ohmzio and Ampwerks. Leave the IAM permissions and other configurations as the default.\nClick Next.\nStep 2, set up data source: Name the knowledge base data source with the name provided (otherwise the default name will be used): merger-knowledge-base-data-source Click Browse S3 to select S3 URI of source data.\nClick customerdata-{uniqueId} bucket name. Click the radio button next to data/ to select it as the location for the source data. Click Choose.\nIf you can\u0026rsquo;t find your bucket, make sure you are in the correct AWS region.\nVerify the S3 URI: s3://customerdata-{uniqueId}/data/ Don\u0026rsquo;t change any of the advanced settings.\nClick Next.\nStep 3, select embeddings model and configure vector store: Select Titan Text Embeddings V2 model. This is the model that will be used to create the vector embeddings in the vector store.\nVector database. Select Quick create a new vector store - Recommended. This is the default value and should already be selected. An Amazon OpenSearch Serverless vector store will be created automatically for you.\nLeave the optional configurations unchecked.\nClick Next. Step 4, Review and create: Review knowledge base configuration.\nVerify the S3 URI is correct:\ns3://customerdata-{uniqueId}/data/ Click Create knowledge base.\nWait for the knowledge base to be created. AWS will prepare the vector database in Amazon OpenSearch Serverless. This may take up to 10 minutes to complete.\nOnce the knowledge base is created, you should see a message at the top of your console:\nClick Go to data source or scroll down to the Data source section. To sync, click the radio button next to the Data source that we created (name should be similar to merger-knowledge-base-quick-start-). Click Sync. This will take a few minutes. Data from your S3 bucket is being read and broken \u0026ldquo;chunks.\u0026rdquo; By default, Amazon Bedrock automatically splits your source data such that each chunk contains 300 tokens. The selected model, Amazon Titan Embeddings V2, is used to convert your data into vector embeddings for the knowledge base.\nWhen the sync is complete, you should receive another message. "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/4-customeraiagent/4.1-createbedrockagent/",
	"title": "Create Bedrock  Agent",
	"tags": [],
	"description": "",
	"content": "Introduction In this lab, you will setup the agent\u0026rsquo;s purpose and select the appropriate foundation model. The knowledge base will be integrated to augment the agent\u0026rsquo;s generative abilities, enabling it to draw upon and synthesize information from product information and customer support tickets.\nSteps Open the Amazon Bedrock console .\nSelect Agents from the left navigation pane. It is under the Builder Tools section, just below Knowledge bases.\nIn the Agents section, choose Create Agent.\nOn the Create Agent screen, enter the name and description of your agent.\nName: customer-support-agent Description: AI agent to retrieve customer information and ticket history to further help support representatives to resolve customer issues. Click Create. After creating the agent, the Agent builder opens for additional configuration. In the Agent details section, complete the following: Verify your Agent name and Agent description that you entered on the previous screen.\nFor the Agent resource role, choose Create and use a new service role so that Amazon Bedrock will create the service role on your behalf.\nOn the Select model section, choose Anthropic and then choose Claude 3 Sonnet in the dropdown menu.\nIn Instructions for the Agent, enter details to instruct the agent on what it should do and how it should interact with users. These instructions will be used in the orchestration prompt template. Here are the instructions to use for the agent: You are a personal assistant for a customer support representative. You will help the support representative retrieve customer details including order and support ticket history, and help the representative resolve customer issues. When the representative asks for help solving a customer\u0026#39;s problem, please do your best to find a resolution. When you don\u0026#39;t have enough information, try to resolve it anyway. Don\u0026#39;t suggest opening a ticket. Never say, \u0026#34;Sorry, I don\u0026#39;t have enough information to answer that.\u0026#34; Instead, do your best to come up with a solution. Expand Additional Settings.\nUnder User input, select the radio button for Enabled. This will allow the agent to prompt the user for additional information. Don\u0026rsquo;t change any other additional settings.\nClick the Save button at the top before moving to the next step. You will receive a message when the agent is successfully updated. Notice the messages to prepare the agent before testing. You will prepare the agent at a later step.\nSkip Action groups for now and scroll down to Knowledge bases.\nIn the Knowledge bases section, select Add to associate a knowledge base with your agent.\nFor Select knowledge base, choose your knowledge base from the dropdown menu. You should see the merger knowledge base you created in an earlier lab.\nFor Knowledge base instructions for the agent, enter instructions to describe how the agent should use the knowledge base:\nUse this knowledge base to find information about products, customers, and how to resolve customer issues. The knowledge base contains product manuals and historical support tickets. Click Add.\nClick Save and exit.\nIn the Agent overview screen, note the entry under Agent ARN. It should look similar to the screenshot below. This is the ARN used to identify your agent. Copy this value for the next section; you will need it in order to grant this agent permission to invoke a Lambda function.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Today, we will be working with sample data from two fictitious organizations.\nOhmzio produces smart home devices: innovative Internet-connected devices like smart lightbulbs, switches, outlets, and sensors to automate the home.\nAmpwerks produces electric vehicle accessories - specialized EV components like battery upgrades, charging systems, adapter cables, power inverters, and control modules to improve the performance of electric vehicles.\nBusiness Challenge Ohmzio, a smart home device company, has acquired Ampwerks, an electric vehicle accessories manufacturer, and now needs to integrate Ampwerks\u0026rsquo; products, customer data, and support systems into their existing business infrastructure in order to effectively support and sell the newly acquired Ampwerks product line. This will require consolidating separate customer and product databases, identifying upsell opportunities across the combined product portfolio, and unifying their support systems to provide a seamless customer experience for both Ohmzio and Ampwerks customers. Effectively integrating operations and data will be critical to realizing the potential benefits of the acquisition.\nGoals Ensure a delightful customer experience as we complete the merger Integrate support systems Create a knowledge base for support engineers to provide customer assistance across all products Leverage generative AI to create an expert chatbot to advise our support engineers\nSolution Overview Steps that have been completed Product data and extracts from both support systems have been stored in an S3 bucket. The product manuals are available as docx files and the support tickets have been extracted as JSON files.\nTo create a consolidated customer record, customer and order data from both Ohmzio and Amperwerks Aurora databases has been extracted and stored in Amazon S3. Amazon Entity Resolution was used to match customers and assign a new global customer identifier.\nThe consolidated customer, order, and support information has been loaded into Amazon DynamoDB for fast retrieval.\nSteps that we will complete in the labs Connect QuickSight to the Ohmzio Aurora database to gain better understanding of products and support incidents.\nUse Amazon Bedrock to build a Knowledge Base containing product and support information. Bedrock deploys Amazon OpenSearch Serverless and uses Amazon Titan models to create vector embeddings of the data.\nAmazon Bedrock Agents are used to interact with the knowledge base and execute lambda functions to retrieve additional customer information.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/2-datadiscovery/2.1-managingquicksightassets/",
	"title": "Managing QuickSight Assets",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, we will be managing QuickSight assets to enable QuickSight users to access data sources that have already been created. These pre-made data sources enable QuickSight to access the source databases.\nSteps Navigate to the Amazon QuickSight console .\nSince this is the initial QuickSight login for this Account/User, please use the following email:\nparticipant-d2e2@amazon.com Take note of the Email you enter before navigating to the next step. This will be used later to share assets and update your QuickSight user role.\nIf opening QuickSight for the first time, users will be presented with What\u0026rsquo;s New in Amazon QuickSight dialogue box. Click Close.\nAccess the QuickSight management console, click the user icon located on the top black ribbon on the right hand side. Click Manage QuickSight from the dropdown menu.\nThis is the QuickSight administrator console. From here, QuickSight administrators can manage users, groups, assets; set account security and permissions to enable QuickSight access to other AWS resources; manage network level resources.\nClick Manage assets on the left menu. Here is where we will enable our user to access the data sources that have been already created.\nIn the Manage assets console, click Data sources in the bottom Browse assets section.\nThe two data sources we need to share are the ohmzio-database and ampwerks-database data sources. Share these by checking the box on the left and then clicking on the SHARE (2) button.\nIn the User or Group field, type out the email (participant-d2e2@amazon.com) noted in the prior step. Click on the username WSParticipantRole/Participant - Admin to populate the field. Click SHARE (2). Once the assets have been shared successfully, click DONE.\nVerify the data sources were shared correctly:\nNavigate back to the QuickSight landing page by clicking the QuickSight icon in the top black ribbon on the left side.\nClick Datasets in the left menu, then click New dataset in the upper right hand corner.\nIn the Create a Dataset landing page, in the bottom section FROM EXISTING DATA SOURCES you should see two option: ohmzio-database and ampwerks-database.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/",
	"title": "My Second WorkShop",
	"tags": [],
	"description": "",
	"content": "Generative AI in Action Description Through a captivating real-world business use case, explore firsthand how generative AI can redefine the way you approach problem-solving and innovation. This workshop consists of step-by-step comprehensive guides and presentation material detailing how to use generative AI to solve real business problems. This lab will take approximately 90 minutes to complete.\nContent Introduction Preparation Support Knowledge Base Customer AI Agent Clean Up "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/1-introduce/1.1-setup/",
	"title": "Set up",
	"tags": [],
	"description": "",
	"content": "Before we start, we need to get a few things set up first. Select the appropriate setup experience:\nInstructor-led Workshop: If you are using an AWS Account provided to you through AWS Workshop Studio, use the AWS Workshop Studio instructions. Review the instructions to ensure your account is setup properly for the lab.\nSelf-paced Workshop: If you are using your own account, follow the steps provided in the Self-Paced Workshop guide to baseline your AWS Account correctly.\nClicking next will take you to the AWS Workshop Studio setup instructions for events hosted by AWS.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/4-customeraiagent/4.2-configuringlambdapermissions/",
	"title": "Configuring Lambda Permissions",
	"tags": [],
	"description": "",
	"content": "Introduction In this lab, you will configure permissions to allow the agent to execute an AWS Lambda function.\nAWS Lambda is an incredibly powerful and versatile serverless computing service. With Lambda, you don\u0026rsquo;t have to worry about provisioning or managing servers. Lambda automatically scales your application in response to increased traffic or demand, ensuring that your application can handle the workload without any manual intervention.\nBy combining AWS Lambda and Agents for Amazon Bedrock, you can build complex, serverless automation and workflow systems. Bedrock agents can be used to trigger Lambda functions, and the Lambda functions can then orchestrate a series of actions, such as updating databases, sending notifications, or invoking other services.\nIn this lab, we will be using our agent to invoke a lambda function that retrieves customer information from an Amazon DynamoDB table.\nAmazon DynamoDB is a powerful and scalable database service that scales automatically to handle any amount of data and traffic, without the need to provision or manage servers. It can handle up to millions of requests per second, with consistent, single-digit millisecond latency.\nFor your agent to use a Lambda function, you must attach a resource-based policy to the function to provide permissions for the agent.\nSteps Open AWS Lambda Functions console . In the list of functions, select the function name BedrockAction. The function to retrieve customer data has already been created and deployed prior to the lab. We won\u0026rsquo;t be changing any of the code, only setting permissions.\nThe Code tab is opened by default. Select Configuration.\nIn the left menu, select Permissions. It should be the third item from the top.\nScroll down to Resource-based policy statements.\nClick Add permissions.\nOn the Add permissions page, configure the following:\nSelect AWS service.\nIn the Service dropdown, select Other.\nIn the Statement ID text box, enter:\nmy-custom-id-0001 In the Principal text box, enter:\nbedrock.amazonaws.com In the Source ARN text box paste the Agent ARN you copied after you created your agent. It should look similar to:\narn:aws:bedrock:us-west-2:123456789012:agent/0ABCDEFGHI In the Action dropdown, select lambda:InvokeFunction.\nClick Save.\nVerify that your Lambda Resource-based policy statement was saved. "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/2-datadiscovery/2.2-creatingquicksightdatasets/",
	"title": "Creating QuickSight Datassets",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, we will be creating a dataset to analyze customer ticket inquiries to help identify opportunities for product improvements using the ohmzio-database data source. After connecting to a new data source, Amazon QuickSight stores the connection information. It adds the data source to the FROM EXISTING DATA SOURCES section of the Create a Dataset page. You can use these existing data sources to create new datasets without re-specifying connection information.\nSteps Navigate to the new dataset console . In the FROM EXISTING DATA SOURCES section of the Create a Dataset page, choose the ohmzio-database, and then choose Create dataset.\nOn the Choose your table screen, connect to ohmziodb schema from Schema: contain sets of tables option and select support_tickets table from Tables: contain the data you can visualize.\nTo prepare the data before creating an analysis, choose Edit/Preview data to open data preparation.\nJoin the support_tickets table with the customers and products tables. Steps are also shown in the animation below.\nClick the Add data button located in the top right corner of the screen.\nSelect Data source from the drop down menu.\nSelect ohmzio-database as the data source.\nSelect ohmziodb as the schema containing the tables.\nClick the checkbox next to the customers and products tables.\nClick the select button to select both tables.\nYou will now see the additional tables added with two red dots indicating that the joins have not been configured. Click each set of red dots to access the join configuration options.\nFor customers, populate the join clause by selecting customer_id as the join column for both the support_tickets and customers tables. Choose the recommended join type of Left and make sure you click the Apply button to save your changes.\nFor products, populate the join clause by selecting product_id as the join column for both the support_tickets and products tables. Choose the recommended join type of Left and make sure you click the Apply button to save your changes. DataSources Assets You can flag geographic fields in your data, so that Amazon QuickSight can display them on a map. On the left side of the Fields pane, look for geographic columns city, state, zip, latitude, and longitude.\nClick the ellipsis menu button ⋮ next to City field. Click Change data type and choose City under the Geospatial data type subsection. Repeat for the following:\nState field will map to State Geospatial data type Zip field will map to Postcode Geospatial data type Latitude field will map to Latitude Geospatial data type Longitude field will map to Longitude Geospatial data type You will notice the field icon will update for each field once it is updated to a geospatial data type.\nTo make your dataset name more intuitive and descriptive change the dataset name from support_tickets to customer_support_discovery. To update, click into the text field with the current title and update with the following: customer_support_discovery Click on Save \u0026amp; Publish (in top blue ribbon, on right side) to save your changes and publish the dataset.\nNavigate back to the QuickSight landing page by clicking the QuickSight icon in the top black ribbon on the left side.\nClick on Datasets in the left menu to see your published dataset customer_support_discovery.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/2-datadiscovery/",
	"title": "Data Discovery",
	"tags": [],
	"description": "",
	"content": "Background As technologists working on the recent Ampwerks and Ohmzio merger, it is up to us to understand the state of each company\u0026rsquo;s data. Both Ampwerks and Ohmzio databases have been migrated to an AWS account. Additionally, our account has been subscribed to Amazon QuickSight.\nIn this lab, we will configure Amazon QuickSight to allow us to explore data from Ampwerks and Ohmzio databases. The data we will use for this lab is a collection of customer, order, and product data. Here is the schema for each of the databases:\nOhmzio Database Schema ohmziodb\r├── products\r│ ├── product_id INT NOT NULL\r│ ├── name VARCHAR(100) NOT NULL\r│ ├── category VARCHAR(50) NOT NULL\r│ ├── description VARCHAR(500) NOT NULL\r│ ├── price DECIMAL(10,2) NOT NULL\r│ └── PRIMARY KEY (product_id)\r├── customers\r│ ├── customer_id VARCHAR(20) NOT NULL\r│ ├── phone VARCHAR(20)\r│ ├── name VARCHAR(100)\r│ ├── address VARCHAR(200)\r│ ├── city VARCHAR(100)\r│ ├── state VARCHAR(100)\r│ ├── zip VARCHAR(10)\r│ ├── dob DATE\r│ ├── company VARCHAR(200)\r│ ├── job VARCHAR(100)\r│ ├── email VARCHAR(100)\r│ ├── latitude DECIMAL(9,6)\r│ ├── longitude DECIMAL(9,6)\r│ └── PRIMARY KEY (customer_id)\r├── support_agents\r│ ├── agent_id INT NOT NULL\r│ ├── fname VARCHAR(50) NOT NULL\r│ ├── lname VARCHAR(50) NOT NULL\r│ └── PRIMARY KEY (agent_id)\r├── order_header\r│ ├── order_id VARCHAR(20) NOT NULL\r│ ├── customer_id VARCHAR(20) NOT NULL\r│ ├── order_date DATE NOT NULL\r│ ├── total DECIMAL(10,2) NOT NULL DEFAULT 0\r│ ├── PRIMARY KEY (order_id)\r│ └── FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\r└── order_items\r├── order_id VARCHAR(20) NOT NULL\r├── product_id INT NOT NULL\r├── unit_price DECIMAL(10,2) NOT NULL\r├── qty INT NOT NULL\r├── item_total DECIMAL(10,2) NOT NULL\r├── PRIMARY KEY (order_id, product_id)\r├── FOREIGN KEY (order_id) REFERENCES order_header(order_id)\r└── FOREIGN KEY (product_id) REFERENCES products(product_id) Ampwerks Database Schema ampwerksdb\r├── products\r│ ├── product_id INT PRIMARY KEY\r│ ├── name VARCHAR(255) NOT NULL\r│ ├── type VARCHAR(255) NOT NULL\r│ ├── price DECIMAL(10,2) NOT NULL\r│ ├── voltage VARCHAR(255)\r│ └── connectors VARCHAR(255)\r├── customers\r│ ├── customer_id VARCHAR(20) NOT NULL\r│ ├── phone VARCHAR(20)\r│ ├── name VARCHAR(100)\r│ ├── address VARCHAR(200)\r│ ├── city VARCHAR(100)\r│ ├── state VARCHAR(100)\r│ ├── zip VARCHAR(10)\r│ ├── dob DATE\r│ ├── company VARCHAR(200)\r│ ├── job VARCHAR(100)\r│ ├── email VARCHAR(100)\r│ ├── latitude DECIMAL(9,6)\r│ ├── longitude DECIMAL(9,6)\r│ └── PRIMARY KEY (customer_id)\r├── support_agents\r│ ├── agent_id INT NOT NULL\r│ ├── fname VARCHAR(50) NOT NULL\r│ ├── lname VARCHAR(50) NOT NULL\r│ └── PRIMARY KEY (agent_id)\r├── order_header\r│ ├── order_id VARCHAR(20) NOT NULL\r│ ├── customer_id VARCHAR(20) NOT NULL\r│ ├── order_date DATE NOT NULL\r│ ├── total DECIMAL(10,2) NOT NULL DEFAULT 0\r│ ├── PRIMARY KEY (order_id)\r│ └── FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\r├── order_items\r│ ├── order_id VARCHAR(20) NOT NULL\r│ ├── product_id INT NOT NULL\r│ ├── unit_price DECIMAL(10,2) NOT NULL\r│ ├── qty INT NOT NULL\r│ ├── item_total DECIMAL(10,2) NOT NULL\r│ ├── PRIMARY KEY (order_id, product_id)\r│ ├── FOREIGN KEY (order_id) REFERENCES order_header(order_id)\r│ └── FOREIGN KEY (product_id) REFERENCES products(product_id)\r└── support_tickets\r├── ticket_id VARCHAR(20) NOT NULL\r├── customer_id VARCHAR(20) NOT NULL\r├── product_id INT NOT NULL\r├── agent_id INT NOT NULL\r├── ticket_date DATE NOT NULL\r├── PRIMARY KEY (ticket_id)\r├── FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\r├── FOREIGN KEY (product_id) REFERENCES products(product_id)\r└── FOREIGN KEY (agent_id) REFERENCES support_agents(agent_id) Steps Log into QuickSight console Manage data source assets to enable access Create new datasets for each database using the QuickSight visual data configuration tool Create visuals to uncover initial customer and product insights for each company "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/1-introduce/1.1-setup/1.1.2-selfpacedworkshop/",
	"title": "Self-Paced Workshop",
	"tags": [],
	"description": "",
	"content": "Introduction We will walk through the steps to get started if you are running the workshop in your own AWS account. If you are being provided an AWS account use the Workshop Studio instructions instead.\nDuration This workshop will take approximately 2-3 hours to complete all activities. If this workshop is being performed in an AWS Account that is owned and paid for by the user, it is strongly recommended to set aside a block of time to complete these activities within a single session. Once the session is complete, be sure to follow the Cleanup instructions to spin-down AWS services and infrastructure deployed for the labs.\nPricing Price estimations to complete all workshop activities:\nDeploying the necessary `ds-genai-workshop CloudFormation template and completing all activites as prescribed in this workshop will incur $1.50 - $4.00 per hour charge ($35-$45 per day) and a $250 one-time fee. Please see below for details.\nAs described below, underlying AWS services and infrastructure are necessary to make this workshop possible. Once the foundational infrastructure is deplyed, your account will begin to incur charges. The price of the workshop will range between $1.50 - $4.00 per hour. It is recommended to set aside a couple of hours in order to complete the workshop and reduce charges incurred for idle resources. Once the labs are complete, be sure to follow the Cleanup i instructions to spin-down AWS services and infrastructure deployed for the workshop.\nPrerequisites The technical architecture for this workshop is desgined to make certain activities accessible and concise. In order to achieve this, a CloudFormation template is provided to deploy baseline infrastructure. In order to deploy the template successfully ensure all lab activities and services are accessible, the following pre-requisites must be met:\nVerify Amazon Bedrock availability in your AWS Region\nAmazon Bedrock, and other AWS services used in this workshop, are currently supported in certain AWS Regions. It is recommended lab activities are run in us-west-2 (Oregon). See Supported AWS Regions for Amazon Bedrock for availability details.\nExisting Default VPC\nAmazon EC2, Amazon RDS, and many other services deployed as part of the baseline infrastructure require an Amazon Virtual Private Cloud or more commonly referred to as a VPC. A VPC is a logical representation of virtual networking environments in your AWS Account. Every AWS Account is initially provided a \u0026ldquo;Default\u0026rdquo; VPC. This lab assumes that the \u0026ldquo;Default\u0026rdquo; VPC in the account exists. See below for instructions on how to check for an existing Default VPC.\nExisting QuickSight subscription\nAmazon QuickSight is used for several activities in this workshop. The baseline infrastructure deployed in the CloudFormation template is designed to create a new QuickSight subscription for users if one doesn\u0026rsquo;t already exist. If you are unsure whether your account has an existing QuickSight subscription, see below for instructions on how to check for an existing QuickSight subscription.\nChecking for existing Default VPC As mentioned above, the CloudFormation template that deploys baseline infrastructure relies on the \u0026ldquo;Default\u0026rdquo; VPC existing in the AWS Account being used. A \u0026ldquo;Default\u0026rdquo; VPC is provided to all AWS Accounts upon creation for all AWS Regions. To verify the Default VPC exists in the AWS Account for the desired regions, follow these steps:\nNavigate to the AWS VPC Dashboard\nIn the right menu, under the Virtual private cloud section, click Your VPCs.\nThis is a list of all VPCs that exist in the account for the region specified. From this console users can manage network configurations, security, and other services from the VPC console.\nIn the list, check to see if a VPC exists that is named Default. If there is a VPC named Default in this list, the account has the necessary infrastructure to deploy successfully. If the Default VPC does not exist, check in another accessible region that supports Amazon Bedrock (see above). If there is not an alterative region, please create a new AWS Account . Checking for existing QuickSight subscriptions In this workshop, users leverage QuickSight for various lab activities. The baseline template provided makes the workshop environment accessible by deploying underlying infrastructure for users. One of the services is a QuickSight subscription. We need to provide CloudFormation logic to correctly handle the creation of a QuickSight subscription if one doesn\u0026rsquo;t already exist and deletion if one does exist. If you are unsure whether or not your account has an existing QuickSight subscription, follow these steps:\nNavigate to AWS CloudShell .\nRetrieve your Account Id. This is a 12 digit number that uniquely identifies a specific AWS account. Copy your AWS Account Id from the AWS console by clicking the down arrow next to the username in the upper-right of the screen. In the new window, click copy icon next to the Account ID:.\nPaste the following command in the shell window, be sure to update the command with your AWS Account Id:\naws quicksight describe-account-subscription --aws-account-id \u0026lt;Your_Account_Id\u0026gt; If QuickSight has never been used in this account, you will recieve a ResourceNotFoundException similar to the following: An error occurred (ResourceNotFoundException) when calling the DescribeAccountSubscription operation: The account does not exist. If this is the case, select False for the ExistingQuickSightSubscription parameter.\nIf QuickSight has been used before, but does not have an active subscription you will recieve a response similar to the following: {\r\u0026#34;Status\u0026#34;: 200,\r\u0026#34;AccountInfo\u0026#34;: {\r\u0026#34;AccountName\u0026#34;: \u0026#34;d2e2Test\u0026#34;,\r\u0026#34;Edition\u0026#34;: \u0026#34;ENTERPRISE_AND_Q\u0026#34;,\r\u0026#34;NotificationEmail\u0026#34;: \u0026#34;d2e2@amazon.com\u0026#34;,\r\u0026#34;AuthenticationType\u0026#34;: \u0026#34;IDENTITY_POOL\u0026#34;,\r\u0026#34;AccountSubscriptionStatus\u0026#34;: \u0026#34;UNSUBSCRIBED\u0026#34;\r},\r\u0026#34;RequestId\u0026#34;: \u0026#34;7fda318f-10ab-4907-a4bb-d64cf9a21b3b\u0026#34;\r} Notice the AccountSubscriptionStatus response for this method shows UNSUBSCRIBED. As mentioned prior, a QuickSight subscription was previously active in this account, but is not currently active. If this is the case, select False for the ExistingQuickSightSubscription parameter.\nIf QuickSight has an active subscription, you will recieve a response similar to the following: {\r\u0026#34;Status\u0026#34;: 200,\r\u0026#34;AccountInfo\u0026#34;: {\r\u0026#34;AccountName\u0026#34;: \u0026#34;d2e2Test\u0026#34;,\r\u0026#34;Edition\u0026#34;: \u0026#34;ENTERPRISE_AND_Q\u0026#34;,\r\u0026#34;NotificationEmail\u0026#34;: \u0026#34;d2e2@amazon.com\u0026#34;,\r\u0026#34;AuthenticationType\u0026#34;: \u0026#34;IDENTITY_POOL\u0026#34;,\r\u0026#34;AccountSubscriptionStatus\u0026#34;: \u0026#34;ACCOUNT_CREATED\u0026#34;\r},\r\u0026#34;RequestId\u0026#34;: \u0026#34;7fda318f-10ab-4907-a4bb-d64cf9a21b3b\u0026#34;\r} Notice the AccountSubscriptionStatus response for this method shows ACCOUNT_CREATED. If this is the case, select True for the ExistingQuickSightSubscription parameter. This will instruct Cloudformation to leave the existing QuickSight subscription when the stack is deleted.\nIf you do not have sufficient permissions to access QuickSight, you may encounter an error similar to the following: Insufficient permissions\nError details\u0026hellip;\nIf this is the case, please reach out to your AWS or IT administrator to obtain sufficient user privileges to run this lab.\nDeployment Steps Download the ds-genai-workshop.yaml from the workshop repository: ds-genai-workshop.yaml .\nDeploy the ds-genai-workshop.yaml template:\nNavigate to the AWS CloudFormation Console .\nFor Prerequisite - Prepare template, keep the default setting: Choose an existing template.\nUnder Specify template, select Upload a template file. Click Choose file to upload the ds-genai-workshop.yaml template downloaded previously.\nIn Specify stack details: Provide ds-genai-workshop as a Stack name and update the ExistingQuickSightSubscription parameter. For additional details about verifying your account for existing QuickSight subscriptions, see above. CloudFormation will use this to determine whether or not it should remove the QuickSight subscription when the stack is deleted. If this is incorrectly designated, CloudFormation will attempt to delete any pre-existing active QuickSight subscriptions.\nFor Configure stack options, keep default settings and click Next.\nLastly Review and create, scroll to the bottom and check the acknowledgement box and click Submit.\nWait for the CloudFormation stack to deploy.\nOnce the template has completed, proceed to the next lab.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/3-supportknowledgebase/3.2-testknowledgebase/",
	"title": "Test the Knowledge Base",
	"tags": [],
	"description": "",
	"content": "Introduction Now that we have created a knowledge base that will perform RAG on our support tickets and product data for both Ohmzio and Ampwerks, we will use prompt engineering to test generative AI capabilities based on the data provided.\nPrompt Engineering As defined in the section introduction, prompt engineering is the method of designing effective prompts that guide the model to produce desired outputs. Here are some best practices to consider when exploring the data source with the knowledge base:\nBe specific: Specificity helps the model understand the context and deliver more accurate responses. For example, instead of asking \u0026ldquo;tell me about Ohmzio Smart Bulbs,\u0026rdquo; ask \u0026ldquo;what are the most common issues with Ohmzio Smart Bulbs?\u0026rdquo;\nUse keywords: Incorporate relevant keywords into your prompt that signal the context or domain you\u0026rsquo;re interested in. This helps the model to tune its responses according to the desired topic.\nStructure prompts: For complex queries, structure your prompt in a logical, step-by-step manner. You might start with a brief introduction to the topic, followed by a specific question or request for explanation.\nIncorporate examples: When looking for creative or complex outputs, providing examples within your prompt can guide the model towards the kind of response you\u0026rsquo;re seeking.\nSteps Navigate to the Amazon Bedrock console .\nClick Knowledge bases in the left menu under the Builder Tools section.\nClick the radio button next to the merger-knowledge-base we created in the previous section. Then click Test knowledge base.\nIn the Test knowledge base window, click Select model. In the pop-up window, select Claude 3 Sonnet and click Apply. In a previous lab, you used Amazon QuickSight to discover that Smart Plugs have the highest number of support incidents. Let\u0026rsquo;s use the knowledge base to uncover additional insights about those support incidents. In the panel with the input box, copy the following prompt, and click Run: What are the most common issues with Ohmzio Smart Plugs? Here is a sample generative response:\nBe aware that the output generated by Knowledge Base may not exactly match the samples provided in the lab guide. As the undifferentiated heavy lifting of training, tuning, and overall improvement of abstracted infrastructure is continuously updated to provide the most performant and reliable experience for end users.\nNote that the response in the console includes references to source details. Source details can be accessed by clicking Show source details at the end of each response. This allows greater transparency for claims the model may make in a given responses.\nWhat actions can you take to avoid calls to support? Use the knowledge base to gain even more insights. How can we update our documentation to avoid some of these unnecessary calls to support? The knowledge base contains product manuals and support ticket history. Based on this information, the LLM produces actions that can be taken to improve documentation and reducing the number of support calls.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/4-customeraiagent/4.3-addingactiongroup/",
	"title": "Adding Action Group",
	"tags": [],
	"description": "",
	"content": "Introduction Action groups can be defined to further specialize the agent\u0026rsquo;s functionality, allowing it to perform specific actions or procedures. Actions can also retrieve additional information, providing the agent with additional knowledge and capabilities necessary to generate relevant and coherent responses.\nSteps Return to your Amazon Bedrock Agents console .\nSelect your agent from the list.\nSelect Edit in Agent Builder.\nScroll down to Action groups.\nBedrock may have created a default action group (\u0026ldquo;UserInputAction\u0026rdquo;) for you. If so, click on the UserInputAction action group and follow the steps below to edit it. If you do not have any existing action group, click Add to create a new one.\nClick Add.\nOn the Create (or Edit) action group page, complete the following:\nSee screenshot below\nEnter action group name: agent-action-group Description: Actions taken for the agent. For Action group type, choose Define with function details.\nFor Action group invocation, choose Select an existing Lambda function.\nFor Select Lambda function, choose BedrockAction and $LATEST version.\nAdd action group function.\nEnter the details for Action group function 1. Name: GetCustomerDetails Description: Retrieve customer by customer id and return the customer details, order history, and support history. Confirmation should be Disabled.\nAdd parameter\nClick the check after editing each parameter attribute.\nName: cust_id Description:\nThe ID of the customer. Type\nString Required\nTrue Click Save and exit. Verify that the action group was created. Scroll back to the top and click Save and exit.\nVerify that the agent was updated. Agent updated\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/3-supportknowledgebase/3.3-conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations! You have just completed setting up a Product and Support Knowledge base!\nSummary In this lab, you:\nLearned the basics of Retrieval-Augmented Generagion (RAG) and prompt engineering.\nCreated an Amazon Bedrock Knowledge Base.\nDeployed Vector Engine for Amazon OpenSearch Serverless\nUsed Amazon Titan to create vector embeddings.\nTested the knowledge base by interacting with a Large Language Model (LLM). Next Steps When you created the knowledge base, on OpenSearch Serverless vector store was created.\nExplore the collection to monitor performance. OpenSearch Serverless Collections\nAdditional Learning Retrieval-Augmented Generagion\nKnowledge Bases for Amazon Bedrock\nVector Engine for Amazon OpenSearch Serverless\nAmazon Titan Text Embeddings\nUnlocking the Value of Generative AI for Business Leaders\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/2-datadiscovery/2.3-ohmzioquicksightdashboard/",
	"title": "Ohmzio QuickSight Dashboard",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, we will create an analysis for analyzing the types of inquiries and issues customers are raising, which can help identify areas for improvement in products and services.\nSteps Navigate to the QuickSight Datasets page.\nClick the ellipsis menu button ⋮ next to customer_support_discovery dataset, and then choose CREATE ANALYSIS.\nOn the New sheet pop-up, leave Interactive sheet selected and click the CREATE button.\nLet\u0026rsquo;s start by analyzing the volume of unique support tickets received on a monthly basis.\nClick the empty visual and change the visual type to line chart. From the fields list in primary left pane, drag ticket_date to the visual\u0026rsquo;s X axis field in right pane and ticket_id to the value field of the visual.\nBy default, you will see that ticket_date is aggregated by day. Click the ellipsis against ticket_date field from the X AXIS section of the visual to access the visual-level field menu to change the aggregation from day to month.\nClick the ellipsis against ticket_id field from the VALUE section of visual to access the visual-level field menu to change the aggregation from count to count distinct to get count of unique tickets.\nThe analysis of historical data reveals overall an upward trend in ticket volume over the months, indicating an increasing demand for support services.\nNext let\u0026rsquo;s view the distribution of support requests across various product types. Choose Add on the application bar, and then choose Add visual. A new, blank visual is created.\nClick the empty visual and change the visual type to Donut chart. From the Fields list in primary left pane, drag category to the Group/Color field well and ticket_id to Value field well.\nThe analysis show Lighting category has most volume of tickets.\nNext let\u0026rsquo;s view which product has most number of support tickets. Choose Add on the application bar, and then choose Add visual. A new, blank visual is created and receives focus.\nClick the empty visual and change the visual type to Tree Map.\nFrom the Fields list in primary left pane, drag name[products] to the GROUP BY field well and ticket_id to Color field well. You may see name[customers] and not name[products]. Because both the customers and the products tables have a column titled “name”, Quicksight renames one of them. Make sure you select name[products] or just name if you only see name[customers]. The bracket notation for products indicates that the attribute originates from the products table. When joining tables that have the same column names, QuickSight resolves this by adding the table name. If you joined support_tickets with the products table before you joined with the customers table, your column names will be different.\nThe analysis shows Ohmzio Smart Plug product has most volume of tickets.\nNext let\u0026rsquo;s add suggested insights. From the top left, choose Insights icon (light bulb shaped icon). The Insights panel opens and displays a list of ready-to-use suggested insights. Scroll down to preview more insights.\nAdd a suggested insight (such as, Top 3 Products by Ticket Count) to your analysis by choosing the plus sign (+) near the insight title.\nYou can resize the insights to be smaller, and move them on top of your Tree Map visual.\nThe auto-generated insights reveal that the Smart Plug and Smart Vent products have the most volume of tickets.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/3-supportknowledgebase/",
	"title": "Product and Support Knowledge Base",
	"tags": [],
	"description": "",
	"content": "Background After uncovering key initial findings in the data using descriptive analytics in QuickSight, lets leverage generative AI capabilities to augment the data discovery process. To ensure a smooth transition during the merger, understanding pain points customers experience with products is paramount. We will be using a combination of AWS generative AI services and prompt engineering best practices to generate outputs.\nPrompt Engineering Prompt engineering is a technique used in the field of artificial intelligence (AI), particularly with generative AI models such as Large Language Models (LLMs), to craft inputs (prompts) that guide the model to produce desired outputs. It involves designing, refining, and testing prompts to effectively communicate the task or question to the model in a way that maximizes the likelihood of receiving accurate, relevant, and coherent responses.\nBy applying best practices in engineering prompts used in the knowledge base, we can receive optimal high-value outputs. Additionally, effective prompt engineering requires an understanding of the model\u0026rsquo;s training data, capabilities, and limitations. Prompts must be specific enough to elicit the desired response but flexible enough to allow the model to generate creative or insightful outputs.\nProduct and Support Data You have been provided product and support data for both Ohmzio and Ampwerks. The data is currently stored in an Amazon S3 Bucket.\ns3://customerdata\r└── data/\r├── ampwerks/\r│ ├── productinfo/\r│ │ └── \u0026lt;product-documentation\u0026gt;.docx\r│ └── support/\r│ └── \u0026lt;support-tickets\u0026gt;.json\r└── ohmzio/\r├── productinfo/\r│ └── \u0026lt;product-documentation\u0026gt;.docx\r└── support/\r└── \u0026lt;support-tickets\u0026gt;.json For each respective company, the productinfo/ directory contains .docx files for each product they sell. Each document contain comprehensive descriptions of the product, instructions, and FAQs.\nThe support/ directory contains .json files for every customer support ticket. These support tickets contain the interations between a support representative and the customer requesting for support about a product they purchased.\nThe productinfo/ and support/ data will provide highly-relevent contextual data to the foundational model. This will enable the model to provide responses based on real interactions and reduce hallucinations.\nhallucination: (related to artifical intelligence) is a response generated by AI which contains false or misleading information presented as fact.\nAmazon Bedrock: Knowledge Bases To augment discovery efforts, you will use Amazon Bedrock: Knowledge Bases to create a highly specified data source (product and support data for both companies) to perform Retrieval Augmented Generation (RAG) with a Foundation Model (FM) on the data source we define. Here is a logical architecture of the RAG archictecture using Knowledge Bases and key concepts:\nRetrieval Augmented Generation (RAG) is a methodology in artificial intelligence that combines the capabilities of two main components: a retrieval system and a generative model. This approach is particularly used in the context of natural language processing (NLP) tasks, such as question answering, content generation, and conversation systems. The objective of RAG is to enhance the quality, relevance, and accuracy of the generative model\u0026rsquo;s outputs by dynamically incorporating external knowledge retrieved in real-time.\nLarge Language Model (LLM) is a very large deep learning model that is pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. This allows LLMs to be incredibly flexible. One model can perform completely different tasks such as answering questions, summarizing documents, translating languages and completing sentences.\nFoundational Model (FM) is broader and encompasses not just language models but any type of deep learning model that is pre-trained on a large dataset and can be fine-tuned for a variety of tasks. This concept extends beyond text to include models trained on images, audio, and more.\nVector Store in the context of generative AI refers to a database or storage system that holds high-dimensional vectors, which are representations of data items (such as text, images, or audio) in a form that machines can efficiently process and understand. These vectors are typically generated through the process of embedding, where data items are converted into vectors of fixed size, capturing the semantic properties and relationships of the original data in a continuous vector space.\nWhen used with RAG, a generative AI model can query a vector store to retrieve relevant information based on the input query. This information can then be used to generate responses that are both relevant and informed by up-to-date or specific data.\nSteps Create Amazon Bedrock: Knowledge Base Test knowledge base Use prompt engineering to extract data about products and services "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/2-datadiscovery/2.4-conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations! You have just completed the Data Discovery lab.\nSummary In this lab, you:\nConfigured Amazon QuickSight and shared two datasources, allowing direct query of Amazon Aurora databases.\nCreated Amazon QuickSight datasets, joining multiple source tables.\nBuilt an Amazon QuickSight dashboard for understanding customer issues.\nNext Steps Now that you have created a dashboard for Ohmzio, check your QuickSight knowledge by creating a dashboard for Ampwerks. Explore different visuals to gain further understanding of the data. Which Ampwerks products are responsible for the most support tickets?\nFor inspiration, check out the Amazon QuickSight Gallery\nAdditional Learning QuickSight Demo Central allows you to experience other QuickSight dashboards. Explore new features and learn tips and tricks.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/4-customeraiagent/",
	"title": "Customer AI Agent",
	"tags": [],
	"description": "",
	"content": "Background The new Ohmzio-Ampwerks IT department has been working to merge the customer databases into a single, consolidated view of customers. They have extracted customer information from both databases and they have leveraged the power of AWS Entity Resolution to match customers and assign a unique customer identifier. The resulting consolidated customer information has been stored in Amazon DynamoDB for fast retrieval from our application.\nObjective Agents for Amazon Bedrock can assist with a wide variety of tasks. For example, you can create an agent that helps customers process insurance claims or an agent that helps customers make travel reservations. Today, you will configure an agent to help the support team provide a better customer experience.\nThis lab walks you through the process of using Agents for Amazon Bedrock to add more capabilities to your conversational AI application. Using agents, you will adjust the behavior of your application by providing specific agent instructions. You will also configure your agent to retrieve customer information to further assist with support cases.\nLab Steps\nCreate an agent for Amazon Bedrock. Add action groups to the agent. Test the agent and view the trace information. An agent consists of the following components:\nFoundation model. You choose a foundation model (FM) that the agent invokes to interpret user input and subsequent prompts in its orchestration process. The agent also invokes the FM to generate responses and follow-up steps in its process.\nInstructions. You write instructions that describe what the agent is designed to do. With advanced prompts, you can further customize instructions for the agent at every step.\nAction groups. You define the actions that the agent should perform through providing the following resources:\nAn OpenAPI schema to define the API operations that the agent can invoke to perform its tasks. A Lambda function that the agent can execute. Knowledge bases. Associate knowledge bases with an agent. The agent queries the knowledge base for extra context to augment response generation and input into steps of the orchestration process.\nPrompt templates. Prompt templates are the basis for creating prompts to be provided to the FM. Agents for Amazon Bedrock exposes the default four base prompt templates that can be edited to customize your agent\u0026rsquo;s behavior.\nThis customizable and extensible agent architecture empowers users to create intelligent assistants tailored to their unique needs and requirements.\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/4-customeraiagent/4.4-testingbedrockagent/",
	"title": "Testing Bedrock Agent",
	"tags": [],
	"description": "",
	"content": "Introduction After you create an agent, you will have a working draft that you can use to iteratively build your agent. Each time you make changes to your agent, the working draft is updated.\nWhen the agent build is complete, you can create a version and an alias. You can then deploy your agent to your applications by calling the alias.\nTesting your working draft In the Amazon Bedrock console, you open up a test window and provide inputs to generate agent responses.\nTo help troubleshoot your agent\u0026rsquo;s behavior, Agents for Amazon Bedrock provides the ability to view the trace during a session with your agent. The trace shows the agent\u0026rsquo;s step-by-step reasoning process.\nSteps During the previous activity, you added the action group for your agent and you should still be on the agent details page. If not, return to your Amazon Bedrock Agents console and select your agent from the list.\nThe Test window should be in a pane on the right. If the Test window is closed, you can reopen it by selecting Test at the top of the agent details page.\nBefore testing the agent, you must prepare it. In the Test window, select Prepare.\nEvery time you update the working draft, you must prepare the agent to package the agent with your latest changes. As a best practice, we recommend that you always check your agent\u0026rsquo;s Last prepared time in the Agent overview section of the Working draft page to verify that you\u0026rsquo;re testing your agent with the latest configurations.\nTo test the agent, enter a message and choose Run. Let\u0026rsquo;s start with a simple greeting: Hi While you wait for the response to generate or after it is generated, select Show trace. Trace shows details for each step of the agent\u0026rsquo;s orchestration process, including the prompt, inference configurations, and agent\u0026rsquo;s reasoning process for each step and usage of its action groups and knowledge bases.\nIn the Orchestration and knowledge base tab, Expand Trace Step 1 and scroll down. Notice the additional context provided and how the prompt is engineered to create the best response from the model. When provided with a the simple greeting, the agent responds with a simple greeting back to the user.\nAsk the agent to find information about a customer using the customer ID. What can you tell me about customer 274877906944? As the agent is working, click Show trace again to view the new tracing data.\nIn the Orchestration and knowledge base, Trace Step 2, note how the agent decided to invoke the function.\nIn the final response, you should see the details for this customer, including products purchased and support tickets from both Ampwerks and Ohmzio. Remember, your output may look different based on variances in the large language models. 7. Ask for more information to help the customer with an issue.\nThe customer is having more issues with the Smart Pet Feeder. The smart feeder is dispensing inconsistent amounts of food. Click Show trace again to view the new tracing data.\nNotice how the agent uses the customer information as well as related support tickets from the knowledge base to create a response.\nAlso notice the [1] in the response. Click the footnote to show the knowledge base document used to find the answer. In this instance, the agent was able to find relevant information from past Ohmzio support tickets.\nContinue the dialog. The customer checked the chute and there are no jams. What else could it be? The agent responds with additional troubleshooting tips, including a suggestion for recalibration. You can respond by asking for more instructions.\nHow do I recalibrate the smart feeder? The recalibration worked. The feeder is now dispensing the correct amount of food. Experiment by asking a few more questions. To start a new conversation, use additional customer IDs that can be found in the DynamoDB table. Here are a few to try: 292057776128\r412316860425\r120259084303 "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/5-cleanup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "Instructions If this lab was run in an account not provided by AWS, follow these steps to tear-down the workshop resources.\nRemove Bedrock Agent and Action Groups created in 4.1 Creating a Bedrock Agent. Remove Bedrock Knowledge Base created in 3.1 Create a Knowlege Base. Remove workshop sample data \u0026ndash; (Optional) Navigate to the S3 Console , and delete the bucket created by the workshop template. The name of the bucket will be customerdata-. If keeping the source data is preferred, this step can be skipped. If this step is skipped, the S3 bucket containing all sample data will remain in the account. This will accrue additional ~24 MB of storage to existing S3 storage costs.\nRemove baseline infrastructure \u0026ndash; Navigate to CloudFormation and delete the ds-genai-workshop stack deployed for the workshop. "
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/4-customeraiagent/4.5-conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "Congratulations! You have just completed setting up an Agent for Amazon Bedrock!\nSummary In this lab, you:\nCreated an agent for Amazon Bedrock. Provided instructions and configured the agent to use your knowledge base. Added an action group to the agent. Configured a lambda function to invoke from your agent. Tested the agent and analyzed tracing information. Next Steps To explore additional customer data to use as agent inputs, you can browse the Amazon DynamoDB table directly. Use this link to browse the all-customers table .\nIn the lab, you configured an action group to call a Lamdba function. The sample lamba function retrieved additional customer information from Amazon DynamoDB and returned it to the agent for additional customer-specific context.\nAmazon Bedrock Agents empower you to construct autonomous agents that can perform tasks on your behalf. Agents automatically call the necessary APIs to transact with your systems and processes to fulfill the request, determining along the way if they can proceed or if they need to gather more information.\nHow can you use Agents to automate tasks within your organization?\nAdditional Learning What is Retrieval-Augmented Generation?\nAgents for Amazon Bedrock\nHow Agents for Amazon Bedrock works\n"
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://dinhlam310.github.io/workshop2.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]